{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09d2fca",
   "metadata": {},
   "source": [
    "# Building Neural Networks and Backpropagation from Scratch\n",
    "\n",
    "In this project, I implemented a **feed-forward neural network** completely from scratch in Python to understand how **backpropagation** works at the mathematical and coding level.\n",
    "\n",
    "Rather than using frameworks like TensorFlow or PyTorch, I derived and coded every step manually (forward pass, loss computation, and gradient backpropagation) to gain an intuition for how neural networks actually learn.\n",
    "\n",
    "**Highlights:**\n",
    "- Built a multi-layer neural network using only NumPy  \n",
    "- Implemented gradient descent and backpropagation manually  \n",
    "- Verified correctness by tracking loss reduction over time  \n",
    "- Wrote modular, well-documented code suitable for reuse  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307710a",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Preparation\n",
    "\n",
    "Import libraries and prepare the data that the network will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c76fe",
   "metadata": {},
   "source": [
    "## Step 2: Defining the Neural Network Architecture\n",
    "\n",
    "Here I define the structure of the neural network - the number of layers, neurons, and the activation functions used.\n",
    "\n",
    "This section sets up the weight matrices and bias vectors, which will later be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dfab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Jacobian for the third layer weights. There is no need to edit this function.\n",
    "def J_W3 (x, y) :\n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # We'll use the variable J to store parts of our result as we go along, updating it in each line.\n",
    "    # Firstly, we calculate dC/da3, using the expressions above.\n",
    "    J = 2 * (a3 - y)\n",
    "    # Next multiply the result we've calculated by the derivative of sigma, evaluated at z3.\n",
    "    J = J * d_sigma(z3)\n",
    "    # Then we take the dot product (along the axis that holds the training examples) with the final partial derivative,\n",
    "    # i.e. dz3/dW3 = a2\n",
    "    # and divide by the number of training examples, for the average over all training examples.\n",
    "    J = J @ a2.T / x.size\n",
    "    # Finally return the result out of the function.\n",
    "    return J\n",
    "\n",
    "#  implement the jacobian for the bias.\n",
    "def J_b3 (x, y) :\n",
    "    # As last time, we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3-y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def J_W2 (x, y) :\n",
    "    #The first two lines are identical to in J_W3.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)    \n",
    "    J = 2 * (a3 - y)\n",
    "    # the next two lines implement da3/da2, first σ' and then W3.\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    # then the final lines are the same as in J_W3 but with the layer number bumped down.\n",
    "    J = J * d_sigma(z2)\n",
    "    J = J @ a1.T / x.size\n",
    "    return J\n",
    "\n",
    "def J_b2 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J*d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill in all incomplete lines.\n",
    "def J_W1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T \n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = J @ a0.T / x.size\n",
    "    return J\n",
    "\n",
    "def J_b1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T \n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3d4d0",
   "metadata": {},
   "source": [
    "##  Step 3: Forward Propagation\n",
    "\n",
    "The forward pass computes the output of the network given an input by applying linear transformations followed by nonlinear activation functions layer by layer.\n",
    "\n",
    "This is where predictions are made before any learning happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9973104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "# First load the worksheet dependencies.\n",
    "# Here is the activation function and its derivative.\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4\n",
    "\n",
    "# This function initialises the network with it's structure, it also resets any training already done.\n",
    "def reset_network (n1 = 6, n2 = 7, random=np.random) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    W1 = random.randn(n1, 1) / 2\n",
    "    W2 = random.randn(n2, n1) / 2\n",
    "    W3 = random.randn(2, n2) / 2\n",
    "    b1 = random.randn(n1, 1) / 2\n",
    "    b2 = random.randn(n2, 1) / 2\n",
    "    b3 = random.randn(2, 1) / 2\n",
    "\n",
    "# This function feeds forward each activation to the next layer. It returns all weighted sums and activations.\n",
    "def network_function(a0) :\n",
    "    z1 = W1 @ a0 + b1\n",
    "    a1 = sigma(z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigma(z2)\n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigma(z3)\n",
    "    return a0, z1, a1, z2, a2, z3, a3\n",
    "\n",
    "# This is the cost function of a neural network with respect to a training set.\n",
    "def cost(x, y) :\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0eca9d",
   "metadata": {},
   "source": [
    "##  Step 4: Backpropagation\n",
    "\n",
    "Here I implemented **backpropagation**.\n",
    "\n",
    "I compute the gradient of the loss with respect to each parameter using the chain rule, moving backward through the network. This tells each weight and bias how to adjust to reduce the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ce383",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def J_W3 (x, y) :\n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # We'll use the variable J to store parts of our result as we go along, updating it in each line.\n",
    "    # Firstly, we calculate dC/da3, using the expressions above.\n",
    "    J = 2 * (a3 - y)\n",
    "    # Next multiply the result we've calculated by the derivative of sigma, evaluated at z3.\n",
    "    J = J * d_sigma(z3)\n",
    "    # Then we take the dot product (along the axis that holds the training examples) with the final partial derivative,\n",
    "    # i.e. dz3/dW3 = a2\n",
    "    # and divide by the number of training examples, for the average over all training examples.\n",
    "    J = J @ a2.T / x.size\n",
    "    # Finally return the result out of the function.\n",
    "    return J\n",
    "\n",
    "def J_b3 (x, y) :\n",
    "    # we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3-y)\n",
    "    J = J * d_sigma(z3)\n",
    "    # For the final line, we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # We still need to sum over all training examples however.\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815404fa",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "Now I bring everything together( forward pass, loss computation, backpropagation, and parameter updates) and run this for multiple epochs to train the network.\n",
    "\n",
    "The loss should gradually decrease, showing that learning is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63be39b",
   "metadata": {},
   "source": [
    "## Step 6: Evaluating and Visualizing the Results\n",
    "\n",
    "After training, I test the model’s accuracy and visualize its learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca26a00",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Writing backpropagation from scratch was a fantastic exercise in both linear algebra and programming logic.\n",
    "\n",
    "**Key takeaways:**\n",
    "- I gained an intuitive understanding of how gradients propagate through a network.  \n",
    "- I learned how small implementation errors (e.g., in matrix shapes) can dramatically affect learning.  \n",
    "- I now have a reusable base for experimenting with other architectures (like convolutional or recurrent networks).\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
